# Hand Tracking CNN, a replacement for the MediaPipe Hand Tracking that's been deprecated since Python3.12
Made with Python and Pytorch <br />
This is a Convolutional Neural Network created to replicate the MediaPipe hands ML model that has deprecated. <br />
`dataset.py` reads the data from the folder into a subclass of the Dataset class from pytorch, overloading the \_\_len\_\_ and \_\_getitem\_\_ functions. It reads the images and files using cv2, os, and json, and converts it to properly formatted data, a \[224, 224, 3\] tensor as an input and a \[42\] tensor for an output. (x, y for 21 points on the hand) <br />
`dataloader.py` is a custom dataloader that is based on the torch.utils.data dataloader. Instead of making it an enumeratable list, it follows the iterator design pattern, returning and moving onto the next value. <br />
`display.py` displays the values using cv2 to display the image and draw the points onto the image. <br />
`model.py` is the CNN, using two sequential layers -- a Conv2d and a Linear -- to convert the image to the corresponding points on the hand. It converts the \[224, 224, 3\] input to a \[28, 28, 128\] tensor through the Convolutional Sequence, then flattens it to a \[128 * 28 * 28, 1\] tensor and finally outputs a \[42, 1\] tensor through the Linear Sequence. <br />
`trainHandDetection.py` is the training logic. It saves (and grabs) from a `.pth` file to load the model, if avaliable, and continues training those weights to fit the data better. Training can be toggled on and off depending on need. It also randomly grabs data from the DataLoader, due to the iterator set to random on creation. This assures that rerunning the file does not continue to train it on the first number of images and all images are trained and tested on. <br />
`view.py` captures images from webcam and classifies it according to the model. It quits on pressing the q key. 
`app.py` creates a FastAPI GET request that allows for calling the model from a frontend. It uses CORS middleware to permit all HTTP methods and headers from this location. The input is a numpy list that is converted to a Pytorch Tensor to use and then returns the tensor with the expected point locations. 

# A note on the training and validation loss
Due to the use of Mean Squared Error, as well as using numbers larger than 1 as a predicted input and output, the loss of the model explodes to very high numbers, fluctuating between 200s to as high as 3000. This is not a large issue, as if we look at the formula for MSE, \( MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \), we can see that the losses are exponentiated, so the loss will be very high for N > 1 and very small for N < 1. <br />
Another interesting note is that, due to the data, after a few iterations the model is incorrectly predicting face as a hand and marking the points on the face. This may be due to the data not having a lot of face + hand pictures, meaning it isn't able to differentiate between two darker areas. As well, the model is unable to predict when there is no hand, as the data does not have instances where hands do not exist. For the future, and what I believe the MediaPipe model does, is that there is a node for either visibility (As in the COCO dataset), or whether it believes it exists or not. I chose to ignore it because 1. all the pictures had visibility of 2, and 2. I was unsure what the model would do with the (x, y) predicted coordinates for hands with 0 visibility, and didn't want it learning from those values. <br />

# Note for the future
The training data does not seem to fit the requirement needed, probably need better data and train it for much longer. The model, after learning from ~10,000 images, seems to predict the location as clustered around the center, and shifts accordingly to the hand, but does not move to the outer areas of the camera when given the input. More training is needed obviously, but the trend is interesting. It doesn't seem to recognize hands yet, but does properly recognize the general clustering of the hand and the rough finger shape. Better data might be required, maybe the Freihand database would have better images. A better CPU/GPU and more storage would be required. Another way would be to increase the dropout rate from 0.5 to 0.7, which would mean it wouldn't be overfitting to the training data and might learn to classify hands over getting a general shape from the images without understanding. <br />

# Data
The data is taken from a Kaggle dataset called "Hand Keypoint Dataset 26K". It is by Rion Dsilva and contains 26,768 images of hands using the MediaPipe library to get the points. <br />
The dataset can be found here: https://www.kaggle.com/datasets/riondsilva21/hand-keypoint-dataset-26k <br />
