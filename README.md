# Hand Tracking CNN, a replacement for the MediaPipe Hand Tracking that's been deprecated since Python3.12
Made with Python and Pytorch <br />
This is a Convolutional Neural Network created to replicate the MediaPipe hands ML model that has deprecated. <br />
`dataset.py` reads the data from the folder into a subclass of the Dataset class from pytorch, overloading the \_\_len\_\_ and \_\_getitem\_\_ functions. It reads the images and files using cv2, os, and json, and converts it to properly formatted data, a \[224, 224, 3\] tensor as an input and a \[42\] tensor for an output. (x, y for 21 points on the hand) <br />
`dataloader.py` is a custom dataloader that is based on the torch.utils.data dataloader. Instead of making it an enumeratable list, it follows the iterator design pattern, returning and moving onto the next value. <br />
`display.py` displays the values using cv2 to display the image and draw the points onto the image. <br />
`model.py` is the CNN, using two sequential layers -- a Conv2d and a Linear -- to convert the image to the corresponding points on the hand. It converts the \[224, 224, 3\] input to a \[28, 28, 128\] tensor through the Convolutional Sequence, then flattens it to a \[128 * 28 * 28, 1\] tensor and finally outputs a \[42, 1\] tensor through the Linear Sequence. <br />
`trainHandDetection.py` is the training logic. It saves (and grabs) from a `.pth` file to load the model, if avaliable, and continues training those weights to fit the data better. Training can be toggled on and off depending on need. It also randomly grabs data from the DataLoader, due to the iterator set to random on creation. This assures that rerunning the file does not continue to train it on the first number of images and all images are trained and tested on. <br />

# A note on the training and validation loss
Due to the use of Mean Squared Error, as well as using numbers larger than 1 as a predicted input and output, the loss of the model explodes to very high numbers, fluctuating between 200s to as high as 3000. This is not a large issue, as if we look at the formula for MSE, \( MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \), we can see that the losses are exponentiated, so the loss will be very high for N > 1 and very small for N < 1. <br />
Another interesting note is that, due to the data, the model is incorrectly predicting face as a hand and marking the points on the face. This may be due to the data not having a lot of face + hand pictures, meaning it isn't able to differentiate between two darker areas. As well, the model is unable to predict when there is no hand, as the data does not have instances where hands do not exist. For the future, and what I believe the MediaPipe model does, is that there is a node for either visibility (As in the COCO dataset), or whether it believes it exists or not. I chose to ignore it because 1. all the pictures had visibility of 2, and 2. I was unsure what the model would do with the (x, y) predicted coordinates for hands with 0 visibility, and didn't want it learning from those values. 

# Data
The data is taken from a Kaggle dataset called "Hand Keypoint Dataset 26K". It is by Rion Dsilva and contains 26,768 images of hands using the MediaPipe library to get the points. <br />
The dataset can be found here: https://www.kaggle.com/datasets/riondsilva21/hand-keypoint-dataset-26k <br />
